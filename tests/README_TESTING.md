# Comprehensive Testing Guide for MetaMCP

This document provides a complete guide to the testing infrastructure for MetaMCP, including test organization, execution, and best practices.

## Table of Contents

1. [Test Structure](#test-structure)
2. [Test Categories](#test-categories)
3. [Running Tests](#running-tests)
4. [Test Infrastructure](#test-infrastructure)
5. [Test Data Management](#test-data-management)
6. [Performance Testing](#performance-testing)
7. [Security Testing](#security-testing)
8. [Integration Testing](#integration-testing)
9. [Test Reporting](#test-reporting)
10. [Best Practices](#best-practices)

---

## Lokale Testumgebung & Mocks

Um reproduzierbare und schnelle Tests zu gewährleisten, beachte bitte folgende Hinweise für die lokale Testumgebung:

- **.env.test verwenden:**
  - Lege eine Datei `.env.test` mit Dummy-Werten für alle Umgebungsvariablen an (z.B. Dummy-API-Keys, SQLite-URL).
  - Beispiel für Datenbank: `DATABASE_URL=sqlite+aiosqlite:///:memory:`

- **SQLite-In-Memory für Unit-Tests:**
  - Für Unit- und Service-Tests empfiehlt sich eine In-Memory-SQLite-Datenbank, um externe Abhängigkeiten zu vermeiden.

- **Externe Services mocken:**
  - Datenbank, Weaviate, LLM und andere externe Systeme sollten in Unit-Tests immer gemockt werden.
  - Nutze dazu z.B. `pytest-mock` oder `unittest.mock.AsyncMock`.
  - Beispiel für das Mocken einer asynchronen DB-Verbindung:
    ```python
    @pytest.mark.asyncio
    async def test_fetch_query(mocker, db_manager):
        mock_pool = mocker.AsyncMock()
        mock_connection = mocker.AsyncMock()
        mock_connection.fetch.return_value = [{"id": 1}]
        mock_pool.acquire.return_value.__aenter__.return_value = mock_connection
        db_manager._pool = mock_pool
        result = await db_manager.fetch("SELECT * FROM test")
        assert result == [{"id": 1}]
    ```

- **Testdaten-Factories:**
  - Nutze die bereitgestellten Factories in `test_data_factory.py` für konsistente Testdaten.

- **Hinweis:**
  - Integration- und Blackbox-Tests benötigen ggf. laufende Services (siehe jeweilige README).

---

## Test Structure

```
tests/
├── __init__.py                 # Main test package
├── conftest.py                 # Comprehensive test configuration and fixtures
├── test_data_factory.py        # Test data factories
├── run_all_tests.py           # Comprehensive test runner
├── README_TESTING.md          # This documentation
├── unit/                      # Unit tests
│   ├── __init__.py
│   ├── test_cli.py           # CLI unit tests
│   ├── test_cli_utils.py     # CLI utilities tests
│   ├── test_services.py      # Service layer tests
│   ├── test_utils.py         # Utility function tests
│   ├── test_health.py        # Health check tests
│   ├── performance/          # Performance tests
│   │   ├── __init__.py
│   │   └── test_performance.py
│   ├── security/             # Security tests
│   │   ├── __init__.py
│   │   └── test_security.py
│   └── telemetry/            # Telemetry tests
│       ├── __init__.py
│       └── test_telemetry.py
├── integration/               # Integration tests
│   ├── __init__.py
│   ├── test_end_to_end.py   # End-to-end workflow tests
│   ├── test_auth.py         # Authentication integration
│   ├── test_cli_integration.py # CLI integration
│   └── test_tools.py        # Tool integration
├── blackbox/                 # Blackbox tests
│   ├── __init__.py
│   ├── conftest.py          # Blackbox test configuration
│   ├── run_tests.py         # Blackbox test runner
│   ├── README.md            # Blackbox test documentation
│   ├── rest_api/            # REST API tests
│   │   ├── __init__.py
│   │   ├── test_auth.py
│   │   ├── test_health.py
│   │   └── test_tools.py
│   ├── mcp_api/             # MCP protocol tests
│   │   ├── __init__.py
│   │   └── test_protocol.py
│   ├── integration/         # Blackbox integration tests
│   │   ├── __init__.py
│   │   └── test_workflows.py
│   └── performance/         # Blackbox performance tests
│       ├── __init__.py
│       └── test_load.py
└── regression/              # Regression tests
    ├── __init__.py
    └── test_example_regression.py
```

## Test Categories

### 1. Unit Tests (`tests/unit/`)

Unit tests verify individual functions, classes, and modules in isolation.

**Key Features:**
- Fast execution (< 1 second per test)
- No external dependencies
- Comprehensive mocking
- High coverage (>90% target)

**Test Files:**
- `test_cli.py`: CLI functionality tests
- `test_services.py`: Service layer tests
- `test_utils.py`: Utility function tests
- `test_health.py`: Health check tests
- `performance/test_performance.py`: Performance benchmarks
- `security/test_security.py`: Security tests
- `telemetry/test_telemetry.py`: Telemetry tests

### 2. Integration Tests (`tests/integration/`)

Integration tests verify component interactions and complete workflows.

**Key Features:**
- Tests component interactions
- End-to-end workflows
- Data consistency verification
- Error propagation testing

**Test Files:**
- `test_end_to_end.py`: Complete workflow tests
- `test_auth.py`: Authentication integration
- `test_cli_integration.py`: CLI integration
- `test_tools.py`: Tool integration

### 3. Blackbox Tests (`tests/blackbox/`)

Blackbox tests verify the system through external interfaces.

**Key Features:**
- Tests public APIs only
- Real HTTP requests
- Container-based testing
- End-to-end scenarios

**Test Categories:**
- `rest_api/`: REST API endpoint tests
- `mcp_api/`: MCP protocol tests
- `integration/`: Cross-component workflows
- `performance/`: Load and stress tests

### 4. Performance Tests

Performance tests measure system performance characteristics.

**Test Types:**
- **Benchmarks**: Baseline performance measurements
- **Load Tests**: Concurrent user simulation
- **Stress Tests**: System limits testing
- **Memory Tests**: Memory usage monitoring

**Metrics Tracked:**
- Response times
- Throughput (requests/second)
- Memory usage
- CPU usage
- Error rates

### 5. Security Tests

Security tests verify security measures and vulnerability protection.

**Test Areas:**
- Authentication and authorization
- Input validation and sanitization
- SQL injection protection
- XSS protection
- Rate limiting
- Token security
- Data encryption

## Running Tests

### Quick Start

```bash
# Run all tests
python tests/run_all_tests.py

# Run specific test categories
pytest tests/unit/                    # Unit tests only
pytest tests/integration/             # Integration tests only
pytest tests/blackbox/               # Blackbox tests only
pytest tests/unit/performance/       # Performance tests only
pytest tests/unit/security/          # Security tests only
```

### Test Execution Options

```bash
# Run with coverage
pytest --cov=metamcp tests/

# Run with verbose output
pytest -v tests/

# Run specific test file
pytest tests/unit/test_services.py

# Run specific test function
pytest tests/unit/test_services.py::TestAuthService::test_authenticate_user

# Run tests with markers
pytest -m "performance" tests/
pytest -m "security" tests/
pytest -m "integration" tests/

# Run tests in parallel
pytest -n auto tests/

# Generate HTML coverage report
pytest --cov=metamcp --cov-report=html tests/
```

### Test Configuration

The main test configuration is in `tests/conftest.py`:

```python
# Test settings
@pytest.fixture(scope="session")
def test_settings() -> Settings:
    return Settings(
        database_url="sqlite:///./test.db",
        secret_key="test-secret-key-for-testing-only",
        testing=True,
        # ... other test-specific settings
    )

# Test fixtures
@pytest.fixture
def auth_service(test_settings: Settings) -> AuthService:
    return AuthService(settings=test_settings)

@pytest.fixture
def tool_service(test_settings: Settings) -> ToolService:
    return ToolService(settings=test_settings)
```

## Test Infrastructure

### Test Data Factory

The `tests/test_data_factory.py` provides comprehensive test data generation:

```python
from tests.test_data_factory import TestDataFactory

factory = TestDataFactory()

# Create test data
user_data = factory.create_user_data()
tool_data = factory.create_tool_data()
search_query = factory.create_search_query()
security_data = factory.create_security_test_data()
```

**Available Factories:**
- `create_user_data()`: User test data
- `create_tool_data()`: Tool test data
- `create_search_query()`: Search query data
- `create_security_test_data()`: Security test payloads
- `create_performance_test_data()`: Performance test data
- `create_load_test_data()`: Load test scenarios

### Test Utilities

The `tests/conftest.py` provides comprehensive test utilities:

```python
# Test utilities
@pytest.fixture
def test_utils():
    return TestUtils()

# Usage
def test_example(test_utils):
    # Measure execution time
    result, execution_time = test_utils.measure_time(some_function)
    
    # Assert response
    data = test_utils.assert_success_response(response)
    
    # Create test file
    test_file = test_utils.create_test_file("test content")
```

## Test Data Management

### Test Data Types

1. **User Data**: Authentication and user management tests
2. **Tool Data**: Tool registration and execution tests
3. **Search Data**: Search functionality tests
4. **Security Data**: Vulnerability testing payloads
5. **Performance Data**: Load and stress test scenarios

### Data Isolation

Each test creates its own test data and cleans up after execution:

```python
@pytest.fixture
async def test_user(auth_service):
    """Create a test user and clean up after test."""
    user_data = create_user_data()
    user = await auth_service.create_user(user_data)
    yield user
    # Cleanup happens automatically
```

### Test Data Factories

The test data factory provides consistent, realistic test data:

```python
# Create specific test data
calculator_tool = factory.create_calculator_tool_data()
search_tool = factory.create_search_tool_data()
admin_user = factory.create_admin_user_data()
security_payloads = factory.create_security_test_data()
```

## Performance Testing

### Performance Test Structure

```python
@pytest.mark.performance
@pytest.mark.asyncio
async def test_authentication_performance(setup_performance_services):
    """Test authentication performance."""
    
    # Measure execution time
    user, creation_time = await self.measure_async_execution_time(
        self.auth_service.create_user, user_data
    )
    
    # Performance assertions
    assert creation_time < 1.0  # Should complete within 1 second
    assert memory_increase < 50  # Memory increase < 50MB
```

### Performance Metrics

**Response Time Thresholds:**
- User creation: < 1.0 seconds
- Tool registration: < 2.0 seconds
- Tool search: < 1.0 seconds
- Tool execution: < 5.0 seconds
- Authentication: < 0.5 seconds

**Resource Usage Thresholds:**
- Memory increase: < 100MB per operation
- CPU usage: < 50% during normal operations
- Memory leaks: < 200MB after 20 operations

### Load Testing

```python
@pytest.mark.load
@pytest.mark.asyncio
async def test_concurrent_user_registration(self, test_settings):
    """Test concurrent user registration performance."""
    
    # Register 10 users concurrently
    tasks = [register_user(i) for i in range(10)]
    results = await asyncio.gather(*tasks)
    
    assert len(results) == 10
    assert total_time < 5.0  # Should complete within 5 seconds
```

## Security Testing

### Security Test Categories

1. **Authentication Security**
   - Password hashing verification
   - Token security and expiration
   - Brute force protection
   - Account lockout mechanisms

2. **Authorization Security**
   - Permission-based access control
   - Resource ownership verification
   - Role-based access control
   - Inactive user handling

3. **Input Validation Security**
   - SQL injection protection
   - XSS protection
   - Path traversal protection
   - Parameter validation

4. **Rate Limiting Security**
   - Rate limiting protection
   - DDoS protection
   - Abuse prevention

### Security Test Examples

```python
@pytest.mark.security
@pytest.mark.asyncio
async def test_sql_injection_protection(self, setup_validation_security):
    """Test SQL injection protection."""
    
    malicious_usernames = [
        "'; DROP TABLE users; --",
        "' OR '1'='1",
        "'; INSERT INTO users VALUES ('hacker', 'password'); --",
    ]
    
    for malicious_username in malicious_usernames:
        with pytest.raises(Exception):
            await self.auth_service.create_user({
                "username": malicious_username,
                "email": "test@example.com",
                "password": "TestPass123!",
                # ... other fields
            })
```

## Integration Testing

### End-to-End Workflows

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_complete_user_workflow(self, setup_services, async_client):
    """Test complete user workflow from registration to tool execution."""
    
    # 1. User Authentication
    login_response = await async_client.post("/api/v1/auth/login", json={
        "username": "user",
        "password": "user123"
    })
    assert login_response.status_code == 200
    
    # 2. Tool Registration
    register_response = await async_client.post(
        "/api/v1/tools/register",
        json=tool_data,
        headers=headers
    )
    assert register_response.status_code == 200
    
    # 3. Tool Execution
    execution_response = await async_client.post(
        f"/api/v1/tools/{tool_id}/execute",
        json=execution_data,
        headers=headers
    )
    assert execution_response.status_code == 200
```

### Cross-Component Integration

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_auth_tool_integration(self, test_settings):
    """Test integration between authentication and tool services."""
    
    # Create user and token
    user = await auth_service.create_user(user_data)
    token = await auth_service.create_access_token(token_data)
    
    # Register tool with user context
    tool_id = await tool_service.register_tool(tool_data, user["username"])
    
    # Verify tool access with user context
    tool = await tool_service.get_tool(tool_id, user["username"])
    assert tool is not None
```

## Test Reporting

### Comprehensive Test Runner

The `tests/run_all_tests.py` provides comprehensive test execution and reporting:

```bash
python tests/run_all_tests.py
```

**Features:**
- Runs all test suites
- Generates detailed reports
- Creates coverage reports
- Performs security scanning
- Provides recommendations

### Report Types

1. **Coverage Reports**
   - HTML coverage report: `test-results/coverage/html/index.html`
   - XML coverage report: `test-results/coverage.xml`
   - Terminal coverage summary

2. **JUnit XML Reports**
   - Unit tests: `test-results/unit-tests.xml`
   - Integration tests: `test-results/integration-tests.xml`
   - Performance tests: `test-results/performance-tests.xml`
   - Security tests: `test-results/security-tests.xml`
   - Blackbox tests: `test-results/blackbox-tests.xml`

3. **Security Scan Reports**
   - Security scan: `test-results/security-scan.json`
   - Vulnerability analysis

4. **Performance Reports**
   - Performance metrics
   - Benchmark results
   - Load test results

### Report Analysis

The test runner provides:

```python
# Summary statistics
{
    "test_suites": {
        "total": 9,
        "successful": 8,
        "failed": 1,
        "success_rate": 88.9
    },
    "total_execution_time": 45.2,
    "recommendations": [
        "Fix failing test suites: security_tests",
        "Increase test coverage (currently 75.2%)"
    ]
}
```

## Best Practices

### Test Organization

1. **Descriptive Test Names**
   ```python
   def test_authenticate_user_with_valid_credentials_returns_user():
       # Test implementation
   ```

2. **Arrange-Act-Assert Pattern**
   ```python
   def test_example():
       # Arrange
       user_data = create_user_data()
       
       # Act
       user = auth_service.create_user(user_data)
       
       # Assert
       assert user is not None
       assert user["username"] == user_data["username"]
   ```

3. **Test Isolation**
   ```python
   @pytest.fixture(autouse=True)
   async def cleanup_test_data():
       yield
       # Cleanup happens automatically
   ```

### Test Data Management

1. **Use Test Data Factories**
   ```python
   from tests.test_data_factory import create_user_data, create_tool_data
   
   user_data = create_user_data(username="testuser")
   tool_data = create_tool_data(name="TestTool")
   ```

2. **Avoid Hardcoded Data**
   ```python
   # Good
   user_data = factory.create_user_data()
   
   # Bad
   user_data = {"username": "test", "password": "123"}
   ```

### Performance Testing

1. **Set Realistic Thresholds**
   ```python
   assert execution_time < 1.0  # Realistic threshold
   assert memory_increase < 50  # Reasonable limit
   ```

2. **Measure Multiple Metrics**
   ```python
   start_memory = self.get_memory_usage()
   start_cpu = self.get_cpu_usage()
   
   # Execute operation
   
   end_memory = self.get_memory_usage()
   end_cpu = self.get_cpu_usage()
   
   assert end_memory["rss_mb"] - start_memory["rss_mb"] < 50
   assert end_cpu - start_cpu < 20
   ```

### Security Testing

1. **Test All Attack Vectors**
   ```python
   security_data = factory.create_security_test_data()
   
   for payload in security_data["sql_injection_payloads"]:
       with pytest.raises(Exception):
           # Test should reject malicious input
   ```

2. **Verify Security Headers**
   ```python
   response = await client.get("/api/v1/health")
   assert "X-Content-Type-Options" in response.headers
   assert "X-Frame-Options" in response.headers
   ```

### Continuous Integration

1. **Automated Test Execution**
   ```yaml
   # .github/workflows/ci.yml
   - name: Run tests
     run: |
       python tests/run_all_tests.py
   ```

2. **Coverage Requirements**
   ```python
   # Minimum coverage threshold
   assert coverage_percentage >= 80.0
   ```

3. **Performance Regression Detection**
   ```python
   # Store baseline metrics
   baseline_time = 1.0
   current_time = measure_execution_time(operation)
   assert current_time <= baseline_time * 1.2  # 20% tolerance
   ```

## Testabdeckung & Verbesserungsplan

### Aktueller Status (Stand: Dezember 2024)

**Testabdeckung:** ~44% Gesamtabdeckung
- **Hohe Abdeckung (>80%):** `metamcp/api/tools.py`, `metamcp/services/auth_service.py`
- **Niedrige Abdeckung (<20%):** `metamcp/client.py`, `metamcp/composition/engine.py`, `metamcp/proxy/manager.py`

**Identifizierte Probleme:**
- Viele Tests schlagen fehl aufgrund fehlender externer Services (DB, Weaviate, LLMs)
- Pydantic V1-Warnungen (Validatoren, Config-Klassen)
- Fehlende Testdaten und Umgebungsvariablen
- Unvollständige Mocking-Strategien für externe Dependencies

---

### Verbesserungsplan

#### Phase 1: Sofortmaßnahmen (1-2 Tage) ✅
- [x] **Pydantic-Migration:** V1-Validatoren (`@validator`) → V2 (`@field_validator`)
- [x] **Config-Migration:** `Config`-Klassen → `ConfigDict`
- [x] **Test-Dokumentation:** README um lokale Testumgebung & Mocks erweitert
- [x] **Mock-Fixtures:** Universelle Fixtures für DB, Weaviate, LLM in `conftest.py`

#### Phase 2: Testinfrastruktur & Mocking (1 Woche)
- [ ] **Testdaten-Factories erweitern:**
  - Komplexe Szenarien für Composition Engine
  - Realistische MCP-Protokoll-Daten
  - Edge Cases für Error-Handling
- [ ] **Service-Layer Tests:**
  - Vollständige Unit-Tests für `metamcp/services/`
  - Mocking aller externen Dependencies
  - Error-Szenarien und Edge Cases
- [ ] **API-Layer Tests:**
  - FastAPI Endpoint Tests mit TestClient
  - Request/Response Validation
  - Authentication & Authorization Tests

#### Phase 3: Kernmodule-Abdeckung (2 Wochen)
- [ ] **Client-Module:** `metamcp/client.py` (aktuell <20%)
  - Connection Management Tests
  - Protocol Handling Tests
  - Error Recovery Tests
- [ ] **Composition Engine:** `metamcp/composition/engine.py` (aktuell <20%)
  - Workflow Execution Tests
  - Tool Composition Tests
  - State Management Tests
- [ ] **Proxy Manager:** `metamcp/proxy/manager.py` (aktuell <20%)
  - Proxy Lifecycle Tests
  - Load Balancing Tests
  - Failover Scenarios

#### Phase 4: Integration & E2E Tests (1 Woche)
- [ ] **End-to-End Workflows:**
  - Komplette MCP-Sessions
  - Tool-Chain-Execution
  - Error-Propagation-Tests
- [ ] **Performance-Baselines:**
  - Response-Time-Messungen
  - Memory-Usage-Tracking
  - Throughput-Tests

#### Phase 5: Qualitätssicherung (1 Woche)
- [ ] **Coverage-Ziele erreichen:**
  - Mindestens 80% Gesamtabdeckung
  - 90% für kritische Module (Auth, API, Services)
  - 70% für Utility-Module
- [ ] **Test-Performance optimieren:**
  - Test-Suite unter 5 Minuten
  - Parallele Test-Ausführung
  - Caching von Testdaten

---

### Erfolgsmetriken

**Quantitativ:**
- Testabdeckung: 44% → 80%+
- Test-Ausführungszeit: <5 Minuten
- Fehlgeschlagene Tests: <5%

**Qualitativ:**
- Reproduzierbare Tests (keine flaky tests)
- Klare Test-Dokumentation
- Einfache lokale Testausführung
- Automatisierte CI/CD-Integration

---

### Nächste Schritte

1. **Sofort:** Phase 1 abschließen (Pydantic-Migration)
2. **Diese Woche:** Phase 2 beginnen (Testdaten-Factories)
3. **Nächste Woche:** Phase 3 starten (Kernmodule-Tests)
4. **Monitoring:** Wöchentliche Coverage-Reports

---

## Troubleshooting

### Common Issues

1. **Test Dependencies**
   ```bash
   pip install pytest pytest-asyncio pytest-cov faker psutil
   ```

2. **Database Issues**
   ```python
   # Use in-memory database for tests
   database_url = "sqlite:///:memory:"
   ```

3. **Mock Issues**
   ```python
   # Proper mocking
   with patch("module.function") as mock_function:
       mock_function.return_value = expected_value
       # Test implementation
   ```

### Debug Mode

```bash
# Run with debug output
pytest -v -s --tb=long tests/

# Run single test with debug
pytest tests/unit/test_services.py::TestAuthService::test_authenticate_user -v -s
```

### Performance Debugging

```python
# Enable performance debugging
import logging
logging.getLogger("performance").setLevel(logging.DEBUG)

# Monitor resource usage
import psutil
process = psutil.Process()
print(f"Memory: {process.memory_info().rss / 1024 / 1024:.1f} MB")
print(f"CPU: {psutil.cpu_percent()}%")
```

This comprehensive testing infrastructure ensures high code quality, security, and performance for the MetaMCP project. 